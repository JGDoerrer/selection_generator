\only<beamer>{\titleframe}

\author{SEA 2025 \hspace{1cm} Johanna Hofmann}
\section{Introduction}

\begin{frame}{The Selection Problem}
  \framesubtitle{Finding the $i$-th Smallest Element}
  \only<1->{\textbf{Selection($n,i$):} Find the $i$-th smallest element in a collection of $n$ distinct values.}
  
  \vspace{5mm}

  \centering
  \only<2->{Common approaches are based on sorting or partitioning:}
  
  \vspace{5mm}
  \begin{columns}[T]
    \begin{column}{0.45\textwidth}
      \centering
      \only<3->{
      \textbf{Sorting}\\
      \includegraphics[width=0.8\textwidth, trim={0 0 0 3.5cm}, clip]{figures/Insertion_sort.jpg}\\
      \tiny{Source: medium.com/@teamtechsis}
        }
    \end{column}
    \begin{column}{0.45\textwidth}
      \centering
      \only<4->{
      \textbf{Pivoting}\\
      \includegraphics[width=0.8\textwidth]{figures/Pivot.jpg}\\
      \tiny{Source: geeksforgeeks.org}
      }
    \end{column}
  \end{columns}

  \note{
    \begin{itemize}
      \item We concerned ourselves with finding exact lower bounds for the number of comparisons in the selection problem
      \item First of all, what is the Selection Problem?
      \item It can be defined by the task of finding the $i$-th smallest element in a collection of $n$ distinct values.
      \item All baseline algorithms either rely on the principle of sorting or pivoting. 
      \item I suppose you are familiar with the bubble sort algorithm or insertion sort. With sorting the list first and then just picking the selection problem can be solved in a trivial way.
      \item On the other hand, with pivoting one can leverage the advantages of divide-and-conquer and apply the well-known Quickselect algorithm or the median of medians appraoch.
    \end{itemize}
  }
\end{frame}

\begin{frame}{Complexity of the Selection Problem}
  \framesubtitle{Known Asymptotic Bounds}
  \begin{itemize}
    \item<+-> Let $V_i^n$ be the worst-case number of comparisons for an optimal algorithm.
    \item<+-> Selecting the minimum is simple: $V_1(n) = n-1$.
    \item<+-> The best-known upper bound for selecting the median is $\approx 2.95n$ comparisons.
    \item<+-> The best-known lower bound for the median is $\approx 2.41n$ comparisons.
  \end{itemize}
  \note{
    \begin{itemize}
      \item By counting the comparisons during runtime one can simultaneously obtain the number of comparisons required, which is also the main time complexity measure.
      \item Let's now have a look at some known asymptotic bounds of the selection problem.
      \item First of all, let $V_i^n$ be the worst-case number of comparisons for an optimal algorithm.
      \item E.g. a lower bound for the general selection problem, is the number of comparisons needed for selecting the smallest element.
      \item That is just $n-1$
      \item The hardest element to determine is the median.
      \item The best-known lower bound for the median is $\approx 2.41n$ comparisons.
      \item The best-known upper bound for selecting the median is $\approx 2.95n$ comparisons.

    \end{itemize}
  }
\end{frame}

\begin{frame}{Complexity of the Selection Problem}
  \begin{figure}
    \includegraphics[height=0.7\textheight]{figures/bounds_diagram.png}
    \caption{\small Complexity Bounds for the Selection Problem}
  \end{figure}
  \note{
    \begin{itemize}
      \item Let's visualize that
      \item The green line represents for the numer of comaparions for selecting the smallest element
      \item The orange line represents the lower bound for the median
      \item The blue line represents the upper bound for the median
      \item We are intereseted in the gap between these existing bounds for small n's
      \item We explore, how many comparisons are needed for larger i's in contrast to $i=1$
      \item And also establish optimal algorithms for all $i$ and $n$ and therefore tighten the gap between upper and lower bounds.
    \end{itemize}
  }
\end{frame}

\begin{frame}{Complexity of the Selection Problem}
  \begin{figure}
    \includegraphics[height=0.5\textheight]{figures/bounds_diagram.png}
    \caption{\small Complexity Bounds for the Selection Problem}
  \end{figure}
  \textbf{Our Question:} What are the \textit{exact} values of $V_i^n$ for small $n$ and arbitrary $i$, and how do they compare to these general bounds?
  \note{
    \begin{itemize}
      \item This is summarized by the question
      \item What are the \textit{exact} values of $V_i^n$ for small $n$ and arbitrary $i$, and how do they compare to these general bounds?
      \item Asymptotic analysis, provide useful insights into the growth behavior of a problem. 
      \item For small instances exact values can be obtained with fully automated decision tree analysis.
      \item This brings us to most relevant previous work on this topic.
    \end{itemize}
  }
\end{frame}

\begin{frame}{Previous Work on Exact Bounds}
  \begin{tabular}{|p{6cm}|p{6cm}|}
    \hline
    \multicolumn{1}{|c|}{Gasarch, Kelly and Puth (1996)}                                               & \multicolumn{1}{c|}{Oksanen (2002)} \\
    \cline{1-2}
    \raggedright \begin{itemize}
                   \item [...]were the first to apply computer search to find lower bounds for the selection problem.
                 \end{itemize} &
    \begin{itemize}
      \item[...] published a computer search algorithm that continues the work of Gasarch et al. for larger $n$ and $i$
    \end{itemize}                                        \\
    \cline{1-2}
  \end{tabular}

  \note{
    \begin{itemize}
      \item Gasarch, Kelly and Puth were the first to apply computer search to find lower bounds for the selection problem.
      \item A few years later Oksanen published a computer search algorithm that obtained results for larger $n$ and $i$.
      \item Results are available on his website, but have not been published in any scientific outlet. 
      \item This lack of scientific documentation led us to revisit the problem ourselves.
      \item  We reimplemented the minimax algorithm and improve upon it by introducing suitable heuristics. 
      \item Additionally we performed backward search, which is another significant improvement over the minimax algorithm.
      \item I will start with Forward Search and Julius will continue with the very interesting backward search. 
    \end{itemize}
  }
\end{frame}

% \section{Grundlagen}

% \subsection{Partial Order Sets}

% \sectionframe{\insertsection}

% \begin{frame}{Zentrale Datenstruktur: Poset}
%   \textbf{Definition:}
%   \vspace{1mm}
%   \begin{itemize}
%     \item Sei $\Omega$ eine Menge mit paarweise verschiedenen Elementen mit $\vert \Omega \vert = n$.
%     \item $(n,i,R)$ bezeichnet eine Instanz des \textit{Selection}-Problems.
%     \item $R \subseteq \Omega^2$ mit $(a,b) \in R \Leftrightarrow a \leq b$.
%   \end{itemize}
%   \vspace{2mm}
% \end{frame}

% \begin{frame}{Zentrale Datenstruktur: Poset}
%   \textbf{Dualität:} \\
%   \vspace{1mm}
%   Das Duale eines Posets $P = (n, i, R)$ ist das Poset $P^{-1} = \left(n, n-1-i, R'\right)$ mit $R' = \{ (u,v) \; \vert \; (v,u) \in R\}$.

%   \pause
%   \vspace{2mm}
%   \textbf{Reduktion:} \\
%   \vspace{1mm}
%   Ein Poset heißt \textit{reduziert}, wenn es zu jedem Element höchstens $i$ viele kleinere Elemente gibt und höchstens $n-1-i$ viele größere Elemente.
% \end{frame}

% \begin{frame}{Zentrale Datenstruktur: Poset}
%   \textbf{Kanonische Form:} \\
%   \vspace{1mm}
%   Ein Poset heißt \textit{kanonifiziert}, wenn alle Elemente des Posets kanonisch angeordnet sind. Das heißt, dass alle möglichen Permutationen der Elemente auf das Poset abbilden.

%   \pause
%   \vspace{2mm}
%   \textbf{Normalform:} \\
%   \vspace{1mm}
%   Ein reduziertes und kanonifiziertes Poset heißt \textit{normalisiert}.

%   \pause
%   \vspace{2mm}
%   \textbf{Optimale Kosten:} \\
%   \vspace{1mm}
%   \textit{Kosten eines Posets} $V(P)$ mit $P=(n,i,R)$ sind die optimale Anzahl an weiteren Vergleichen, die benötigt werden, um das $i$ kleinste Element zu bestimmen.
% \end{frame}

% \begin{frame}{Ein Lemma}
%   \textbf{Lemma 1:} Für ein Poset $P=(n,i,R)$ gilt $V(P) = V(P^{-1})$.
%   \vspace{1mm}

%   \textit{Beweis: Die Berechnung von $V(P)$ ergibt gleichzeitig einen Algorithmus, der in einen Entscheidungsbaum übersetzt werden kann. Vertausche alle Kinder, und derselbe Algorithmus führt auf die Lösung von $P^{-1}$.}

% \end{frame}

% \begin{frame}{Ein Beispiel mit Poset-Reduktion}
%   \begin{figure}
%     \input{figures/tikz_example}
%   \end{figure}

% \end{frame}

% \subsection{Kompatible Lösungen}
% \begin{frame}{\insertsubsection}
%   \centering
%   \begin{tikzpicture}

%     \node[draw, circle] (l0) at (0, 0) {};
%     \node[draw, circle] (l1) at (1, 0) {};
%     \node[draw, circle] (l2) at (2, 0) {};
%     \node[draw, circle] (l3) at (3, 0) {};

%     \node[draw, circle] (I) at (1.5, 2) {$i$};

%     \path[draw] (l0) to (I);
%     \path[draw] (l1) to (I);
%     \path[draw] (l2) to (I);
%     \path[draw] (l3) to (I);

%     \node[draw, circle] (u0) at (0.5, 4) {};
%     \node[draw, circle] (u1) at (1.5, 4) {};
%     \node[draw, circle] (u2) at (2.5, 4) {};

%     \path[draw] (u0) to (I);
%     \path[draw] (u1) to (I);
%     \path[draw] (u2) to (I);

%   \end{tikzpicture}

% \end{frame}

% \begin{frame}{\insertsubsection}
%   \centering
%   \begin{tikzpicture}

%     \node[draw, circle] (fl0) at (0, 0) {};
%     \node[draw, circle] (fl1) at (1, 0) {};
%     \node[draw, circle] (fl2) at (2, 0) {};
%     \node[draw, circle] (fl3) at (3, 0) {};

%     \node[draw, circle] (fI) at (1.5, 2) {$i$};

%     \path[draw] (fl0) to (fI);
%     \path[draw] (fl1) to (fI);
%     \path[draw] (fl2) to (fI);
%     \path[draw] (fl3) to (fI);

%     \node[draw, circle] (fu0) at (0.5, 4) {};
%     \node[draw, circle] (fu1) at (1.5, 4) {};
%     \node[draw, circle] (fu2) at (2.5, 4) {};

%     \path[draw] (fu0) to (fI);
%     \path[draw] (fu1) to (fI);
%     \path[draw] (fu2) to (fI);

%     \path[draw=red] (fu0) to (fl1);

%     %second

%     \node[draw, circle] (sl0) at (0 + 5, 0) {};
%     \node[draw, circle] (sl1) at (1 + 5, 0) {};
%     \node[draw, circle] (sl2) at (2 + 5, 0) {};
%     \node[draw, circle] (sl3) at (3 + 5, 0) {};

%     \node[draw, circle] (sI) at (1.5 + 5, 2) {$i$};

%     \path[draw] (sl0) to (sI);
%     \path[draw] (sl1) to (sI);
%     \path[draw] (sl2) to (sI);
%     \path[draw] (sl3) to (sI);

%     \node[draw, circle] (su0) at (0.5 + 5, 4) {};
%     \node[draw, circle] (su1) at (1.5 + 5, 4) {};
%     \node[draw, circle] (su2) at (2.5 + 5, 4) {};

%     \path[draw] (su0) to (sI);
%     \path[draw] (su1) to (sI);
%     \path[draw] (su2) to (sI);

%     \path[draw=red] (su0) to (su1);

%   \end{tikzpicture}

% \end{frame}

% \subsection{Kompatible Lösungen}
% \begin{frame}{\insertsubsection}
%   \centering
%   \includegraphics[scale=.5]{../article/figures/compatible_cost_relation.png}

% \end{frame}




\section{Forward Search}
\sectionframe{\insertsection}

\begin{frame}{\insertsection: A Minimax Approach}
  \centering
  \begin{tikzpicture}
    [
      level 1/.style = {sibling distance = 4cm},
      level 2/.style = {sibling distance = 2.5cm},
    ]

    \node[anchor=east] at (-7,0) {Min};
    \node[anchor=east] at (-7,-1.5) {Max};
    \node[anchor=east] at (-7,-3) {Min};
    \node[anchor=east] at (-7,-4.5) {Max};

    \node[] (root) {$(n,i,\emptyset)$}
    child {
        node[] {$\{a,b\}$}
        child {
            node[] {$(n,i,\{(a,b)\})$}
            child[sibling distance = 1cm] {
                node[] {$\{c,d\}$}
                child[sibling distance = 1cm] { node[] {$\cdots$}}
                child[sibling distance = 1cm] { node[] {$\cdots$}}
              }
            child[sibling distance = 1cm] { node[] {$\cdots$}}
          }
        child {
            node[] {$(n,i,\{(b,a)\})$}
            child[sibling distance = 1cm] {
                node[] {$\{c,d\}$}
                child[sibling distance = 1cm] { node[] {$\cdots$}}
                child[sibling distance = 1cm] { node[] {$\cdots$}}
              }
            child[sibling distance = 1cm] { node[] {$\cdots$}}
          }
      }
    child {
        node {$\{a,c\}$}
        child[sibling distance = 1cm] { node[] {$\cdots$}}
        child[sibling distance = 1cm] { node[] {$\cdots$}}
      }
    child {
        node[] {$\cdots$}
      }
    ;

    \node[anchor=north east] at (7, -2) {
      \parbox{5.5cm}{
        \begin{itemize}
          \item<2-> \textbf{Depth-first search}
          \item<3-> \textbf{Maximum depth} is limited by one less than the current best result
          \item<4-> \textbf{Optimal algorithm} is build by recording the comparisons that led to the cheapest solution
        \end{itemize}
      }
    };

  \end{tikzpicture}
  \note{
    \begin{itemize}
      \item The forward search starts at the root with the initial problem instance which consists of the $n$, the $i$ and an empty set. This set will be filled with all relations resulting from the comparisons.
      \item On the next level, pairs of elements are formed. In the following it will also be called partial-order set or short poset.
      \item For each pair, the two possible outcomes are expanded on the lower levels (i.e. a<b and a>b).
      \item The optimal number of comparisons in the worst case is obtained using the minimax algorithm.
      \item Between all pairs of elements the pair is compared that yields the smallest number of comparisons.
      \item Between the two possible outcomes of a comparison, the worst case is assumed.
      \item To save memory and enable pruning, the search tree is traversed using a depth-first search.
      \item The maximum number of comparisons assigned to child problems is limited to one less than the current best result.
      \item The search program builds algorithms by recording, for each problem, the comparison that led to the cheapest result.
    \end{itemize} }
\end{frame}

\begin{frame}{Optimizations}
  \begin{tikzpicture}
    [
      level 1/.style = {sibling distance = 4cm},
      level 2/.style = {sibling distance = 2.5cm},
    ]

    \node[anchor=east] at (-12,2) {Min};
    \node[anchor=east] at (-12,0.5) {Max};
    \node[anchor=east] at (-12,-1) {Min};
    \node[anchor=east] at (-12,-2.5) {Max};

    \node[] (root) at (-7,2) {$(n,i,\emptyset)$}
    child {
        node[] {$\{a,b\}$}
        child {
            node[] {$(n,i,\{(a,b)\})$}
            child[sibling distance = 1cm] {
                node[] {$\{c,d\}$}
                child[sibling distance = 1cm] { node[] {$\cdots$}}
                child[sibling distance = 1cm] { node[] {$\cdots$}}
              }
            child[sibling distance = 1cm] { node[] {$\cdots$}}
          }
        child {
            node[] {$(n,i,\{(b,a)\})$}
            child[sibling distance = 1cm] {
                node[] {$\{c,d\}$}
                child[sibling distance = 1cm] { node[] {$\cdots$}}
                child[sibling distance = 1cm] { node[] {$\cdots$}}
              }
            child[sibling distance = 1cm] { node[] {$\cdots$}}
          }
      }
    child {
        node {$\{a,c\}$}
        child[sibling distance = 1cm] { node[] {$\cdots$}}
        child[sibling distance = 1cm] { node[] {$\cdots$}}
      };

    \node[anchor=north east] at (3, 1.5) {
      \parbox{7cm}{
        \begin{itemize}
          \item<2-> \textbf{Iterative deepening}
          \item<3-> \textbf{Reduce posets} by removing all elements that cannot be the i-th smallest
          \item<4-> \textbf{Cache}
        \end{itemize}
      }
    };
    \only<4->{\node[anchor=north east] at (1.8,-1.2) {
      \parbox{5cm}{
        \begin{tabular}{|l|c|}
          \toprule
          Poset             & Min \#comp \\
          \midrule
          $(n,i,\{(a,b)\})$ & 1          \\
          $(n,i,\{(b,a)\})$ & 1          \\
          \bottomrule
        \end{tabular}
      }
    };}
  \end{tikzpicture}

  \note{\begin{itemize}
      \item Let's now look at some of the optimizations we applied. 
      \item We apply depth-first search with iterative deepening.
      \item This has significant memory advantages
      \item We work on reduced posets. For each poset we remove elements that cannot be the i-th smallest. So if an element has more than $i-1$ elements that are smaller, or more than $n-i$ elements that are larger, it is removed from the poset.
      \item We store an approximated canonical representation of the problem and its dual in the cache along with the number of comparisons applied so far.
      \item Each entry indicates that the poset is not solvable with just this amount of comparisons.
      \item Besides for simple reusing purposes, the information in the chach can be used for pruning, by cutting off branches that contradict the minimum value in the cache.
    \end{itemize}
  }
\end{frame}

\begin{frame}{Pruning techniques}
\framesubtitle{\large Compatible solutions}
  \only<1->{The solved problem $(S,i)$ is a \textbf{compatible solution} of the problem $(R,i)$, if
  \begin{align*}
    a \leq_S b \Rightarrow b \not \leq_R a
  \end{align*}}
  \vspace*{0.5mm}

  \only<2->{Denote the \textbf{number of compatible solutions} with $\vert \mathcal{C}(P,i) \vert $}
  \vspace{5mm}
  
  \raggedright
  \hspace*{-3mm}
  \only<3->{\begin{tabular}{l p{10cm}}
    \textbf{Lemma: } & Selecting the i-th smallest element of a poset P requires at least $\lceil log (|C(P, i)|) \rceil $ comparisons
  \end{tabular}}
   \note{\begin{itemize}
    \item We applied two further pruning techniques.
    \item The first one I would like to talk about leverages the concept of compatible solution.
    \item We say solved problem is a compatible solution of another problem if there are no contradicting relations.
    \item We denote the \textbf{number} of compatible solutions with the absolute of C(P,i)
    \item The useful lemma related to compatible solutions is the following:
    \item Selecting the i-th smallest element of a poset P requires at least $\lceil log (|C(P, i)|) \rceil $ comparisons
    \item This is because the number of compatible solutions is the number of leaf nodes below the regarded poset.
    \item The logarithm of this the depth of the tree which is equivalent to the number of comparisons needed to achieve the solution.
    \item 
  \end{itemize}}
\end{frame}

\begin{frame}{Pruning techniques}
  \framesubtitle{\large Compatible solutions}
  \only<1>{
    \begin{tikzpicture}
      [
        level 1/.style = {sibling distance = 4cm},
        level 2/.style = {sibling distance = 3cm},
        level 3/.style = {level distance=1.2cm}
      ]

      \node[anchor=east] at (-7,0) {Min};
      \node[anchor=east] at (-7,-1.5) {Max};
      \node[anchor=east] at (-7,-3) {Min};
      \node[anchor=east] at (-7,-4.2) {Max};

      \node[] (root) {$(n,i,\emptyset)$}
      child {
          node[] {$\{a,b\}$}
          child {
              node[] {$(n,i,\{(a,b)\})$}
              child[sibling distance = 1cm] {
                  node[] {$\{c,d\}$}
                  child[sibling distance = 1cm] { node[] {$\cdots$}}
                  child[sibling distance = 1cm] { node[] {$\cdots$}}
                }
              child[sibling distance = 1cm] { node[] {$\cdots$}}
            }
          child {
              node[] {$(n,i,\{(b,a)\})$}
              child[sibling distance = 1cm] {
                  node[] {\color{black}$\{c,d\}$}
                  child[sibling distance = 1cm, edge from parent/.style={draw,dashed}] { node[] {\color{red}42}}
                  child[sibling distance = 1cm] { node[] {$\cdots$}}
                }
              child[sibling distance = 1cm] { node[] {$\cdots$}}
            }
        }
      child {
          node {$\cdots$}
        }
      child {
          node[] {$\cdots$}
        }
      ;

    \end{tikzpicture}
  }
  \only<2>{
    \begin{tikzpicture}
      [
        level 1/.style = {sibling distance = 4cm},
        level 2/.style = {sibling distance = 3cm},
        level 3/.style = {level distance=1.2cm}
      ]

      \node[anchor=east] at (-7,0) {Min};
      \node[anchor=east] at (-7,-1.5) {Max};
      \node[anchor=east] at (-7,-3) {Min};
      \node[anchor=east] at (-7,-4.2) {Max};

      \node[] (root) {$(n,i,\emptyset)$}
      child {
          node[] {$\{a,b\}$}
          child {
              node[] {$(n,i,\{(a,b)\})$}
              child[sibling distance = 1cm] {
                  node[] {$\{c,d\}$}
                  child[sibling distance = 1cm] { node[] {$\cdots$}}
                  child[sibling distance = 1cm] { node[] {$\cdots$}}
                }
              child[sibling distance = 1cm] { node[] {$\cdots$}}
            }
          child {
              node[] {$(n,i,\{(b,a)\})$}
              child[sibling distance = 1cm,red] {
                  node[] {\color{black}$\{c,d\}$}
                  child[sibling distance = 1cm,red, edge from parent/.style={draw,dashed}] { node[] {\color{red}42}}
                  child[sibling distance = 1cm,black] { node[] {$\cdots$}}
                }
              child[sibling distance = 1cm] { node[] {$\cdots$}}
            }
        }
      child {
          node {$\cdots$}
        }
      child {
          node[] {$\cdots$}
        }
      ;

      \node [draw=none] at (-0.5, -3) {\color{red} $(\_, 42)$};
    \end{tikzpicture}
  }
  \only<3>{
    \begin{tikzpicture}
      [
        level 1/.style = {sibling distance = 4cm},
        level 2/.style = {sibling distance = 3cm},
        level 3/.style = {level distance=1.2cm}
      ]

      \node[anchor=east] at (-7,0) {Min};
      \node[anchor=east] at (-7,-1.5) {Max};
      \node[anchor=east] at (-7,-3) {Min};
      \node[anchor=east] at (-7,-4.2) {Max};

      \node[] (root) {$(n,i,\emptyset)$}
      child {
          node[] {$\{a,b\}$}
          child {
              node[] {$(n,i,\{(a,b)\})$}
              child[sibling distance = 1cm] {
                  node[] {$\{c,d\}$}
                  child[sibling distance = 1cm] { node[] {$\cdots$}}
                  child[sibling distance = 1cm] { node[] {$\cdots$}}
                }
              child[sibling distance = 1cm] { node[] {$\cdots$}}
            }
          child {
              node[] {$(n,i,\{(b,a)\})$}
              child[sibling distance = 1cm,red] {
                  node[] {\color{black}$\{c,d\}$}
                  child[sibling distance = 1cm,red,, edge from parent/.style={draw,dashed}] { node[] {\color{red}42}}
                  child[sibling distance = 1cm,black] { node[] {$\cdots$}}
                }
              child[sibling distance = 1cm] { node[] {$\cdots$}}
            }
        }
      child {
          node {$\cdots$}
        }
      child {
          node[] {$\cdots$}
        }
      ;

      \draw[red] (-3.6,-2) -- (-2.9,-2.5);
      \draw[red] (-3.6,-2.5) -- (-2.9,-2);

      \node [draw=none] at (-0.5, -3) {\color{red} $(\_, 42)$};
      \node [draw=none] at (-1, -2.25) {\color{red} If $42 < \lceil \log (\vert \mathcal{C}(P,i)\vert ) \rceil$};


    \end{tikzpicture}
  }

  \note{
    \begin{itemize}
      \item For example when one child turns out to be solvable with 42 comparisons, the value is propagated upwards. And e.g. this is a min node so upon completion the solution is at most 42. But when log of number of compatible solutions is larger than 42 this branch can be pruned.
    \end{itemize}
  }
\end{frame}

\begin{frame}{Pruning techniques}
  \framesubtitle{\large Free comparison}
  \begin{columns}
    \begin{column}{0.65\textwidth}
      \begin{itemize}
      \item<1-> Already applied by Oksanen
      \item<2-> Idea: Add a useful relation between a large element and a small element
      \item<3-> Search for unrelated elements $a$ and $b$ such that $a$ has at least two elements greater than it and $b$ has at least two elements smaller than it
    \end{itemize}
    \end{column}
    \begin{column}{0.35\textwidth}
      \only<3->{\begin{figure}
        \begin{tikzpicture}[node distance=1.2cm]
          \node[draw, circle, minimum size=0.5cm] (1) {b};
          \node[draw, circle, minimum size=0.5cm, below right of=1] (2) {};
          \node[draw, circle, minimum size=0.5cm, below left of=1] (3) {};

          \node[draw, circle, minimum size=0.5cm, above of=1, node distance=1.5cm] (4) {a};
          \node[draw, circle, minimum size=0.5cm, above right of=4] (5) {};
          \node[draw, circle, minimum size=0.5cm, above left of=4] (6) {};

          \path[dashed] (1) edge (4);

          \path[-]
          (1) edge (2)
          (1) edge (3)
          (4) edge (5)
          (4) edge (6);
        \end{tikzpicture}
      \end{figure}}
    \end{column}
  \end{columns}
\end{frame}

\author{SEA 2025 \hspace{1cm} Julius von Smercek}

\section{Backward Search}
\sectionframe{\insertsection}
\note{
  Thank you! Now I would like to continue with the backward search.
}

\begin{frame}{\insertsection}
  Start with a solved poset and search the empty poset
  \vfill
  \begin{itemize}
    \item<+-> \textbf{Goal:} Given an upper bound $k$, find all posets solvable in at most $k$ comparisons.
    \item<+-> \textbf{Initialization:} Start with the set of all posets that are already ``solved'' (i.e., the $i$-th element is uniquely identified). Their cost is 0.
    \item<+-> \textbf{Canonization:} A unique normal form (with `nauty`) is expensive, but required. We use fast invariants to pre-filter, such that `nauty` is needed for only $< 0.1\%$ of posets.
    \item<+-> \textbf{Iteration (Level $k \to k+1$):} To find all posets solvable in $k+1$ steps, find all \textit{predecessors} of posets solvable in $\leq k$ steps.
    \item<+-> \textbf{Termination:} The search stops when the initial empty poset $(n, i, \emptyset)$ is generated. Its level is the value $V_i^n$.
  \end{itemize}
  \note{
    \only<1>{
      In contrast to the forward search, the backward search starts with a solved poset and seeks to find the empty poset with $n$ elements.

      For this, an iterative deepening approach is employed.
      For a given upper bound $k$, this approach finds all posets that can be solved in a maximum of $k$ comparisons.
    }
    \only<2>{
      To achieve this, we start with the set of posets that are already solved, meaning the $i$-th element is uniquely identifiable.
      The cost for each poset in this set is $0$, as they can be solved in $0$ comparisons.

      To avoid redundant computations, we use a normal form that reduces the set of solved posets to a single one: the poset containing only one element for which the minimum is obvious.
      It should be evident that this poset can be solved directly.
    }
    \only<3>{
      This unique normal form is based on leveraging duality and `nauty`.
      While computationally expensive, `nauty` is necessary to guarantee a unique normal form.
      Although an approximate normal form was sufficient for the forward search, a unique normal form is essential for the correctness of the backward search.
      Consequently, we have combined `nauty` with our own procedure, reducing the usage of `nauty` to less than $0.1\%$ of cases.
    }
    \only<4>{
      In each iteration level, we compute all posets that are solvable in $k+1$ steps, given that we know the set of all posets solvable in fewer than $k$ steps.
      To do this, we compute the predecessors of all posets from the previous level.
      I will elaborate on the precise definition of a predecessor shortly.
    }
    \only<5>{
      The search terminates when the empty poset is found.
      The level in which the empty poset with the desired $n$ and $i$ first appears corresponds to the optimal number of comparisons required.
    }
  }
\end{frame}

\begin{frame}{Predecessor Computation}
  \begin{definition}[Predecessor]
    Suppose the set of all posets solvable in $k$ comparisons is known.
    A normalized poset $Q$ is a \textbf{predecessor} iff there exists a comparison between two elements $u,v$ such that
    \begin{enumerate}
      \item<2-> Adding the relation $(u,v)$ to $Q$ yields a known poset solvable in exact $k$ steps
      \item<2-> Adding the relation $(v,u)$ to $Q$ yields a known poset solvable in at maximum $k$ steps
      \item<3-> $Q$ itself was not found at a lower level
    \end{enumerate}
  \end{definition}
  \vfill
  \visible<4>{
    This process essentially ``removes'' a comparison to go one step backward.
  }
  \note{
    \only<1>{
      Okay, let's now define the concept of a predecessor.

      A predecessor is determined by removing a comparison. For this, we assume that the set of all posets solvable in k comparisons is known. This set is therefore a given.
    }
    \only<2>{
      A normalized poset Q is then considered a predecessor if there exists a comparison between two elements, u and v, such that adding the relation (u, v) to Q results in a known poset that is solvable in exactly k steps, and adding the relation (v, u) results in a known poset that is solvable in at most k steps.
    }
    \only<3>{
      This ensures that regardless of the outcome of the comparison, Q is solvable in exactly $k + 1$ comparisons. And, obviously, Q must not have been discovered yet.
    }
    \only<4>{
      This process essentially removes one comparison.
    }
  }
\end{frame}

\begin{frame}{Example}
  \vspace{-0.75cm}
  \begin{figure}[!b]
    \centering
    \scalebox{0.8}{\input{figures/tikz_backward_example.tex}}
    \caption{Search tree for $n=4,i=2$}
    \label{fig:backward-searchtree-bound4}
  \end{figure}

  \note{
    \only<1>{
      I've brought an example to illustrate this.

      We start with the solved poset at the bottom and are looking for the second smallest element in an empty poset with four elements. Obviously, the solved poset is solved in 0 comparisons; for the initial poset at the top, the number of comparisons is still unknown.
    }
    \only<2>{
      The very first thing we do is compute all posets that are solvable in one comparison. In this case, there is only one poset with two elements for which we are searching the minimum. Theoretically, the dual of this poset - that is, the poset with two elements where we seek the maximum - would also be on this level, but since they are isomorphic, one can be omitted.

      The solid arrow here represents the resulting poset if the comparison (u, v) were to be inserted, and the dashed lines represent the resulting poset if the opposite comparison were inserted.
    }
    \only<3>{
      Analogously, we can compute the level for $k = 2$. Here you can see it nicely again: the solid arrow always points to the level directly below it, while the other arrow points to some level further down.
    }
    \only<4>{
      In this way, we can construct the tree until we finally find a path from the empty poset to the solved poset at $k = 4$. This means that this subtree represents an optimal algorithm for finding the second smallest element among four elements.
    }
  }
\end{frame}

\begin{frame}{Removing Comparisons}
  \begin{figure}[!b]
    \centering
    \scalebox{1.5}{\input{figures/tikz_problematic_case.tex}}
    \caption{TODO}
    \label{fig:backward-problematic-case}
  \end{figure}
  \note{
    \only<1>{
      A problem that may already be apparent here is that the removal of comparisons is not unambiguous.
      We are given a poset with three elements, where we know that the bottom element is less than the middle one, and the middle element is less than the top one.
      This edge is invisible in the Hasse diagram.
    }
    \only<2>{
      If we now remove the red comparison between the bottom and the middle element, it is possible that the transitive edge between the top and bottom elements remains.
    }
    \only<3>{
      Consequently, if the transitive edge does not exist, the middle poset could be the result.
      However, if the edge still exists, then the last case would apply.
      The fact that removing a single comparison can lead to multiple possible posets complicates the problem.
    }
  }
\end{frame}

\begin{frame}{Poset Distribution}
  \begin{figure}[!b]
    \centering
    \resizebox{0.9\textheight}{!}{%
      \input{figures/tikz_posets_per_level.tex}
    }
    \caption{Number of posets for $n = 14$ depending on the number of comparisons}
  \end{figure}
  \note{
    Another interesting aspect is the poset distribution for the backward search.
    Here, we have plotted the number of removed comparisons against the number of found posets on a logarithmic scale for different values of $i$.
    It is noticeable that for larger $i$, significantly more posets must be searched.
    Another interesting point is that the peak of searched posets for all $i$ lies between 5 and 10, which will also present a challenge for the bidirectional search.
  }
\end{frame}


\section{Results}
\sectionframe{\insertsection}

\begin{frame}{Exact Bounds} %ready
  \vspace{-1cm}
  \begin{table}[!t]
    \label{tab:results}
    \centering
    \small
    \begin{tabular}{c|cccccccc}
      $n$ & \multicolumn{8}{c}{$i$}                                                                                                      \\
          & 1                       & 2  & 3           & 4           & 5           & 6           & 7                 & 8                 \\ \hline
      1   & 0                                                                                                                            \\
      2   & 1                                                                                                                            \\
      3   & 2                       & 3                                                                                                  \\
      4   & 3                       & 4                                                                                                  \\
      5   & 4                       & 6  & 6                                                                                             \\
      6   & 5                       & 7  & 8                                                                                             \\
      7   & 6                       & 8  & 10          & 10                                                                              \\
      8   & 7                       & 9  & 11          & 12                                                                              \\
      9   & 8                       & 11 & 12          & 14          & 14                                                                \\
      10  & 9                       & 12 & 14          & 15          & 16                                                                \\
      11  & 10                      & 13 & 15          & 17          & 18          & 18                                                  \\
      12  & 11                      & 14 & 17          & 18          & 19          & 20                                                  \\
      13  & 12                      & 15 & 18          & 20          & 21          & 22          & 23                                    \\
      14  & 13                      & 16 & 19          & 21          & 23          & 24          & \textbf{25}                           \\
      15  & 14                      & 17 & 20          & 23          & \textbf{24} & \textbf{26} & \textbf{26}       & \textbf{27}       \\
      16  & 15                      & 18 & \textbf{21} & \textbf{24} & \textbf{26} & \textbf{27} & \textbf{28} -- 33 & \textbf{28} -- 36 \\
    \end{tabular}
  \end{table}
  \note{
    Let us first, however, turn to the results.
    In the presented table, all values that we have newly computed are shown in bold.
    We have, of course, verified all other values.
    In doing so, we were able to improve an existing lower bound, specifically for $n = 15, i = 5$, and prove that the correct lower bound is 24 comparisons.
    For the values $i = 7$ and $8$ at $n = 16$, we could not determine the exact number of comparisons, but we were able to establish a new lower bound.
  }
\end{frame}

\begin{frame}{Contribution}
  \definecolor{counterexample}{rgb}{0.8, 0.1, 0.1}
  \setbeamercolor{block title alerted}{bg=gray!20,fg=black}
  \setbeamercolor{block body alerted}{bg=gray!5}
  \begin{itemize}
    \item<+-> Verified existing results and generated executable algorithms
    \item<+-> Calculated and corrected new lower bounds
    \item<+-> Introduced compatible posets and backward search as novelty for this problem
    \item<+-> Disproved a conjecture on optimal algorithm structure from Gasarch:
      ``An optimal algorithm exists that first compares all elements pairwise.''
      \begin{alertblock}<+->{Counterexample: $n=12, i=5$}
        The conjecture implies a minimum of 20 comparisons are required. \\[0.5em]
        \textbf{Our discovered algorithm succeeds with only \textcolor{counterexample}{19} comparisons.}
      \end{alertblock}
  \end{itemize}
  \note{
    \only<1>{
      To summarize our contribution, we have verified all previous results and generated executable algorithms with the optimal number of comparisons in Rust.
    }
    \only<2>{
      Furthermore, we contribute the new lower bounds as well as the improved value for $n = 15$.
    }
    \only<3>{
      A key component is also the backward search, which was used for the first time for this problem, as well as heuristics, such as the compatible posets, which had not been applied to this problem before.
    }
    \only<4>{
      Finally, we were able to refute a conjecture by Knuth, which was already mentioned as a comment in Oksanen's C program, but could be definitively disproven by our program due to the partial incorrectness of the C code.
    }
    \only<5>{
      He conjectured that an optimal algorithm exists which first compares all elements pairwise.
      We found a counterexample for this, namely for $n=12, i=5$.
      With this assumption, the optimal algorithm requires $20$ comparisons, which does not correspond to the optimal bound of $19$ comparisons.
    }
  }
\end{frame}

\begin{frame}{Runtime Comparison} % ready
  \begin{table}[!t]
    \label{tab:times}
    \renewcommand{\arraystretch}{1.0}
    \centering
    \resizebox{1.0\textheight}{!}{%
      \begin{tabular}{c|c|l|l|l}
        $n$ & $i$ & \textbf{Forward Search} & \textbf{Backward Search} & \textbf{Oksanen}                                         \\
        \hline
        14  & 1   & 0.0s                    & 0.0s                     & 0.0s                                                     \\
        14  & 2   & 0.0s                    & 1.5s                     & 0.0s                                                     \\
        14  & 3   & 1.4s                    & 5.9s                     & \textbf{0.6s}                                            \\
        14  & 4   & \textbf{35.9s}          & 46.9s                    & 1m 47s                                                   \\
        14  & 5   & 17m 27s                 & \textbf{15m 33s}         & 6h 29m                                                   \\
        14  & 6   & 2h 40m                  & \textbf{1h 40m}          & 4d 10h                                                   \\
        14  & 7   & 14h 40m                 & \textbf{6h 27m}          & >5d                                                      \\
        \hline
        15  & 1   & 0.0s                    & 0.0s                     & 0.0s                                                     \\
        15  & 2   & 0.1s                    & 4.0s                     & \textbf{0.0s}                                            \\
        15  & 3   & 2.8s                    & 25.9s                    & \textbf{1.4s}                                            \\
        15  & 4   & \textbf{2m 24s}         & 13m 11s                  & 27m 17s                                                  \\
        15  & 5   & 1h 12m                  & \textbf{45m 52s}         & 1d 5h 40m\footnote{calculates a non-optimal lower bound} \\
        15  & 6   & 1d 8h 37m               & \textbf{19h 30m}         & >5d                                                      \\
        15  & 7   & 4d 23h 37m              & \textbf{1d 5h 43m}       & >5d                                                      \\
        15  & 8   & 14d 1h 51m              & \textbf{3d 8h 9m}        & >5d                                                      \\
      \end{tabular}
    }
  \end{table}
  \note{
    Finally, it is interesting to see how long we had to heat the university to compute these results.
    An important note here is that the forward search is only single-threaded, whereas the backward search is fully parallelized and runs almost linearly faster with the number of threads. With 48 threads and 1 TB of utilized RAM, this makes a significant difference.
    For $n=14$ and $n=15$, I have marked in bold the values that were computed the fastest.
    For very small values, Oksanen's C program is faster; however, for moderate values, the forward search is superior, and for the large values, the backward search is, which is relatively consistent.
    We can see here, for example, that for ($n=15, i=8$), the backward search, taking three days, significantly outperforms the forward search, which would have taken 14 days.
  }
\end{frame}

\begin{frame}{Future Work}
  \vspace{-1.2cm}
  \begin{columns}
    \begin{column}{.49\textwidth}
      \begin{itemize}
        \item<+-> Idea of bidirectional Search: Run forward and backward search simultaneously
        \item<+-> Challenge: The searches intersect only after $> 99.9\%$ of the search space has been explored in both searches
        \item<+-> Forward search can prune subtrees, but the backward search needs to calculate the full tree
      \end{itemize}
    \end{column}
    \begin{column}{.49\textwidth}
      \onslide<2->{
        \begin{figure}[!b]
          \centering
          \renewcommand{\arraystretch}{0.9}
          \resizebox{0.85\textheight}{!}{%
            \input{../article/figures/tikz_backward_forward_13_6.tex}
          }
          \caption{Poset distribution for $n = 13$, $i = 7$}
          \label{fig:backward_forward_count_13_6}
        \end{figure}
      }
    \end{column}
  \end{columns}
  \note{
    \only<1>{
      For the future, we could imagine parallelizing the forward search or developing even better heuristics.
      A bidirectional search would also be conceivable.
      This means we would start the forward and backward searches simultaneously and solve the problem from both ends.
    }
    \only<2>{
      The problem with this approach is that the intersection point of the two searches is only reached after 99.9 percent of the posets have been searched for both directions.
      We have just seen that in the backward search, the search tree explodes very early, which conversely also happens in the forward search.
      We can see this again in this illustrative graph:
      The number of posets for the forward and backward searches are shown in blue and red, respectively.
    }
    \only<3>{
      A possible idea would be to develop pruning strategies for the backward search, although it unfortunately has to search through almost all posets to guarantee correctness.
      These approaches are currently the subject of ongoing research, with the bidirectional search appearing to be the most promising.
    }
  }
\end{frame}

\thanksframe

\note{
  On that note, I would like to thank you for your attention.
  If you have any questions, I would be happy to answer them now.
}

% TODO: prevent in toc above
\begin{frame}[shrink=25]{References}
  \nocite{*}
  \printbibliography[heading=none]
  \note{
    Here are our references, which can also all be found in our paper.
  }
\end{frame}

\begin{frame}[shrink=25]{Image References}
  \begin{itemize}
    \item[] https://medium.com/@teamtechsis/introduction-to-sorting-algorithms-1623b9cdd4f1
    \item[] https://www.geeksforgeeks.org/dsa/quick-sort-algorithm/
  \end{itemize}
\end{frame}

% TODO: unser eigenes Paper